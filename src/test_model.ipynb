{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import softmax\n",
    "\n",
    "def run_inference5():\n",
    "    sent = input(\"Sentence(s) : \")\n",
    "    print(\"predicting...\")\n",
    "    labels = ['上去','下去','下来','出来','起来']\n",
    "    model = torch.load(\"../models/5_mac_large/5_mac_large.tar\")\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "    sent = sent.replace('！','。')\n",
    "    sent = sent.replace('？','。')\n",
    "    sents = sent.split('。')[:-1]\n",
    "    for sent in sents:\n",
    "        sent = sent.strip()\n",
    "        sent_ids = tokenizer(sent)['input_ids']\n",
    "        sent_tensor = torch.tensor(sent_ids).unsqueeze(0).to('cuda')\n",
    "        output = model(sent_tensor)[0]\n",
    "        logits =output.detach().cpu().numpy()[0]\n",
    "        predict = labels[np.argmax(logits, axis=0)]\n",
    "        softmax_output = softmax(output, 1)\n",
    "        print(sent)\n",
    "        print(f\"[MASK] : {predict}\")\n",
    "        result = dict()\n",
    "        for prob, label in zip(softmax_output[0].tolist(), labels):\n",
    "            result[label] = prob\n",
    "        print(f\"\\t出来 : {int(result['出来']*100)}\")\n",
    "        print(f\"\\t起来 : {int(result['起来']*100)}\")\n",
    "        print(f\"\\t上去 : {int(result['上去']*100)}\")\n",
    "        print(f\"\\t下来 : {int(result['下来']*100)}\")\n",
    "        print(f\"\\t下去 : {int(result['下去']*100)}\")        \n",
    "\n",
    "def run_inference8():\n",
    "    sent = input(\"Sentence(s) : \")\n",
    "    print(\"predicting...\")\n",
    "    labels = ['上去','下去','下来','出来','起来','上来','过来','过去']\n",
    "    model = torch.load(\"../models/8_mac_large/8_mac_large.tar\")\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "    sent = sent.replace('！','。')\n",
    "    sent = sent.replace('？','。')\n",
    "    sents = sent.split('。')[:-1]\n",
    "    for sent in sents:\n",
    "        sent = sent.strip()\n",
    "        sent_ids = tokenizer(sent)['input_ids']\n",
    "        sent_tensor = torch.tensor(sent_ids).unsqueeze(0).to('cuda')\n",
    "        output = model(sent_tensor)[0]\n",
    "        logits =output.detach().cpu().numpy()[0]\n",
    "        predict = labels[np.argmax(logits, axis=0)]\n",
    "        softmax_output = softmax(output, 1)\n",
    "        print(sent)\n",
    "        print(f\"[MASK] : {predict}\")\n",
    "        result = dict()\n",
    "        for prob, label in zip(softmax_output[0].tolist(), labels):\n",
    "            result[label] = prob\n",
    "        print(f\"\\t出来 : {int(result['出来']*100)}\")\n",
    "        print(f\"\\t过来 : {int(result['过来']*100)}\")\n",
    "        print(f\"\\t过去 : {int(result['过去']*100)}\")        \n",
    "        print(f\"\\t起来 : {int(result['起来']*100)}\")\n",
    "        print(f\"\\t上来 : {int(result['上来']*100)}\")   \n",
    "        print(f\"\\t上去 : {int(result['上去']*100)}\")\n",
    "        print(f\"\\t下来 : {int(result['下来']*100)}\")\n",
    "        print(f\"\\t下去 : {int(result['下去']*100)}\")\n",
    "\n",
    "def run_inference5_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        sents = f.readlines()\n",
    "    print(\"predicting...\")\n",
    "    labels = ['上去','下去','下来','出来','起来']\n",
    "    model = torch.load(\"../models/5_mac_large/5_mac_large.tar\")\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "\n",
    "    for sent in sents:\n",
    "        sent = sent.strip()\n",
    "        sent_ids = tokenizer(sent)['input_ids']\n",
    "        sent_tensor = torch.tensor(sent_ids).unsqueeze(0).to('cuda')\n",
    "        output = model(sent_tensor)[0]\n",
    "        logits =output.detach().cpu().numpy()[0]\n",
    "        predict = labels[np.argmax(logits, axis=0)]\n",
    "        softmax_output = softmax(output, 1)\n",
    "        print(sent)\n",
    "        print(f\"[MASK] : {predict}\")\n",
    "        result = dict()\n",
    "        for prob, label in zip(softmax_output[0].tolist(), labels):\n",
    "            result[label] = prob\n",
    "        print(f\"\\t出来 : {int(result['出来']*100)}\")\n",
    "        print(f\"\\t起来 : {int(result['起来']*100)}\")\n",
    "        print(f\"\\t上去 : {int(result['上去']*100)}\")\n",
    "        print(f\"\\t下来 : {int(result['下来']*100)}\")\n",
    "        print(f\"\\t下去 : {int(result['下去']*100)}\")        \n",
    "\n",
    "def run_inference8_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        sents = f.readlines()\n",
    "    print(\"predicting...\")\n",
    "    labels = ['上去','下去','下来','出来','起来','上来','过来','过去']\n",
    "    model = torch.load(\"../models/8_mac_large/8_mac_large.tar\")\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "\n",
    "    for sent in sents:\n",
    "        sent = sent.strip()\n",
    "        sent_ids = tokenizer(sent)['input_ids']\n",
    "        sent_tensor = torch.tensor(sent_ids).unsqueeze(0).to('cuda')\n",
    "        output = model(sent_tensor)[0]\n",
    "        logits =output.detach().cpu().numpy()[0]\n",
    "        predict = labels[np.argmax(logits, axis=0)]\n",
    "        softmax_output = softmax(output, 1)\n",
    "        print(sent)\n",
    "        print(f\"[MASK] : {predict}\")\n",
    "        result = dict()\n",
    "        for prob, label in zip(softmax_output[0].tolist(), labels):\n",
    "            result[label] = prob\n",
    "        print(f\"\\t出来 : {int(result['出来']*100)}\")\n",
    "        print(f\"\\t过来 : {int(result['过来']*100)}\")\n",
    "        print(f\"\\t过去 : {int(result['过去']*100)}\")        \n",
    "        print(f\"\\t起来 : {int(result['起来']*100)}\")\n",
    "        print(f\"\\t上来 : {int(result['上来']*100)}\")   \n",
    "        print(f\"\\t上去 : {int(result['上去']*100)}\")\n",
    "        print(f\"\\t下来 : {int(result['下来']*100)}\")\n",
    "        print(f\"\\t下去 : {int(result['下去']*100)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(s) : 春天来了，天气暖和[MASK]了。 春天来了，天气渐渐地暖和[MASK]了。\n",
      "predicting...\n",
      "春天来了，天气暖和[MASK]了\n",
      "[MASK] : 起来\n",
      "\t出来 : 0\n",
      "\t起来 : 99\n",
      "\t上去 : 0\n",
      "\t下来 : 0\n",
      "\t下去 : 0\n",
      "春天来了，天气渐渐地暖和[MASK]了\n",
      "[MASK] : 起来\n",
      "\t出来 : 0\n",
      "\t起来 : 99\n",
      "\t上去 : 0\n",
      "\t下来 : 0\n",
      "\t下去 : 0\n"
     ]
    }
   ],
   "source": [
    "run_inference5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "春天来了，天气暖和[MASK]了。\n",
      "[MASK] : 起来\n",
      "\t出来 : 0\n",
      "\t起来 : 99\n",
      "\t上去 : 0\n",
      "\t下来 : 0\n",
      "\t下去 : 0\n",
      "春天来了，天气渐渐地暖和[MASK]了。\n",
      "[MASK] : 起来\n",
      "\t出来 : 0\n",
      "\t起来 : 99\n",
      "\t上去 : 0\n",
      "\t下来 : 0\n",
      "\t下去 : 0\n"
     ]
    }
   ],
   "source": [
    "run_inference5_file(\"../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
