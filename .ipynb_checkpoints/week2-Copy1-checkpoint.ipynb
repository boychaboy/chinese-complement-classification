{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.19.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: keras in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from transformers import BertForMaskedLM,BertForSequenceClassification\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        l = []\n",
    "        for line in lines:\n",
    "            l.append(line.strip().split(\"</B>\")[1])\n",
    "#             print(line)\n",
    "    return l\n",
    "verb_hao = read_file(\"sample-data/verb_hao.txt\")\n",
    "verb_dao = read_file(\"sample-data/verb_dao.txt\")\n",
    "verb_xialai = read_file(\"sample-data/verb_xialai.txt\")\n",
    "verb_xiaqu = read_file(\"sample-data/verb_xiaqu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'bert-base-chinese'\n",
    "MAX_SIZE = 150\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences) :\n",
    "    sent = []\n",
    "    label = []\n",
    "    for sentence in sentences :\n",
    "        \n",
    "        s1 = sentence.split(\"<U>\")\n",
    "        s2 = s1[1].split(\"</U>\")\n",
    "        label.append(s2[0][-1])\n",
    "        sent.append(\"[CLS]\"+s1[0]+s2[0][:-1]+\"[MASK]\"+s2[1]+\"[SEP]\")\n",
    "        #debug\n",
    "#         print(s2[0][-1]) # label\n",
    "#         print(s1[0]+s2[0][:-1]+\"[MASK]\"+s2[1]) # sent\n",
    "    return sent, label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 예측값의 확률 (Softmax) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_inference(sentences, top_n=5):\n",
    "    predicts = []\n",
    "    probabilities = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        mask_idx = input_ids.tolist()[0].index(103) #103 is [MASK] id\n",
    "        outputs = model(input_ids)\n",
    "        if top_n == 1:\n",
    "            logits = ouptuts[0].detach().numpy()\n",
    "            pred = tokenizer.convert_ids_to_tokens(np.argmax(logits, axis=2)[0])\n",
    "            predicts.append(pred[mask_idx])\n",
    "        else :\n",
    "            logits = outputs[0].detach()\n",
    "            mask_predict = F.softmax(logits[0][mask_idx], dim=0).numpy()\n",
    "            predict_top_n = mask_predict.argsort()[-top_n:][::-1]\n",
    "            predicts.append(tokenizer.convert_ids_to_tokens(predict_top_n))\n",
    "            probabilities.append(mask_predict[predict_top_n].tolist())\n",
    "    return predicts, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['下', '起', '过', '出', '上'] [0.8841641545295715, 0.11500083655118942, 0.0002584183239378035, 9.911341476254165e-05, 7.786255446262658e-05]\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "sents.append([\"这个东西[MASK]贵\"])\n",
    "sents.append([\"教室里突然安静[MASK]来了\"])\n",
    "pred, label = softmax_inference(sents)\n",
    "print(pred[1], label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, label):\n",
    "    cnt = 0\n",
    "    for p, l in zip(pred, label):\n",
    "        if p==l : \n",
    "            cnt += 1\n",
    "    return cnt / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentences):\n",
    "    predicts = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        mask_idx = input_ids.tolist()[0].index(103) #103 is [MASK] id\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[0].detach().numpy()\n",
    "        pred = tokenizer.convert_ids_to_tokens(np.argmax(logits, axis=2)[0])\n",
    "        predicts.append(pred[mask_idx])\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data):\n",
    "    sent, label = preprocessing(data)\n",
    "    predict = inference(sent)\n",
    "    return evaluate(predict, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_softmax(data, top_n = 5, file_name=None):\n",
    "    sent, label = preprocessing(data)\n",
    "    pred, prob = softmax_inference(sent, top_n)\n",
    "    for sentence, candidates, probabilities, label in zip(sent, pred, prob, label):\n",
    "        print(sentence)\n",
    "        for c, p in zip(candidates,  probabilities):\n",
    "            print(\"{} : {:.2f}%\".format(c, p*100))\n",
    "        print(\"Answer : {}\".format(label))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]是约４２ｍｍ的遮光罩镜头外型尺寸为以说一尘不染，可见其密封性恨[MASK]。如此笔者发现独立镜头厂家适配０７２５ｘ１２２ｍｍ光学构造为１[SEP]\n",
      "好 : 43.59%\n",
      "佳 : 20.00%\n",
      "强 : 9.52%\n",
      "高 : 8.08%\n",
      "差 : 3.73%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]很爱听，真像换了个脑筋，明年养多少头牛、羊都一家一户地帮我们算[MASK]了。”东湾乡大泉村二组村民马建雄忙着拉干部到自家去住，“干部住[SEP]\n",
      "账 : 35.00%\n",
      "算 : 20.17%\n",
      "好 : 13.57%\n",
      "计 : 4.53%\n",
      "数 : 4.03%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]有关重大问题。第三，企业档案处置具体工作是收集、整理、统计、保管[MASK]企业已经形成的档案，清点库存，按有关规定做好档案留存与销毁的鉴[SEP]\n",
      "各 : 35.11%\n",
      "本 : 31.27%\n",
      "好 : 7.47%\n",
      "该 : 5.38%\n",
      "出 : 2.22%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]策与财政政策的协调配合还体现为在经济发展的不同时期，两者都要搭配[MASK]。１９９８年下半年，财政为扩大内需，向国有商业银行增加发行１０[SEP]\n",
      "好 : 72.64%\n",
      "住 : 6.52%\n",
      "上 : 5.45%\n",
      "合 : 3.22%\n",
      "着 : 2.45%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]政府贯彻〈中国教育改革和发展纲要〉的意见》。发展教育事业注意处理[MASK]三个关系：在数量和质量的关系上，更加重视质量；在德育和智育的关[SEP]\n",
      "好 : 97.80%\n",
      "了 : 0.46%\n",
      "这 : 0.42%\n",
      "在 : 0.33%\n",
      "的 : 0.32%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]学管理的示范。一是要通过学习，增强创新创业的自觉性。要创业、干[MASK]事业，就必须抓重点、带全局。所有的人力、智力、财力、物力、科技[SEP]\n",
      "好 : 93.50%\n",
      "大 : 3.51%\n",
      "新 : 1.03%\n",
      "实 : 0.44%\n",
      "事 : 0.36%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]解放思想，寻找经济快速发展的新路子。只有人的思想解放了，才能运用[MASK]中央制定的各项方针和政策，才能敢“闯”敢“新”，走出一条独特的[SEP]\n",
      "好 : 92.17%\n",
      "党 : 5.79%\n",
      "住 : 1.31%\n",
      "新 : 0.18%\n",
      "了 : 0.11%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]到手术室，手术室大空间，手术床小范围，手术室到ＩＣＵ等各环节做[MASK]保温，避免外介温度的大波动引起婴儿不良反应。３．３减少患婴术中[SEP]\n",
      "好 : 97.95%\n",
      "到 : 0.87%\n",
      "足 : 0.13%\n",
      "预 : 0.06%\n",
      "的 : 0.06%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]例如，在学习“如何做小主人和小客人”这一社会知识时，我们事先联系[MASK]一个家庭，并向主人详细讲解此次活动的目的、方法、步骤，以取得默[SEP]\n",
      "了 : 32.38%\n",
      "上 : 15.16%\n",
      "好 : 13.83%\n",
      "到 : 10.66%\n",
      "每 : 8.36%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]切实加强农村信用社金融监管与行业管理的初步设想（一）必须正确处理[MASK]三大关系回、正确处理好信用社与联社的关系。基本原则是既要尊重信[SEP]\n",
      "好 : 99.53%\n",
      "这 : 0.27%\n",
      "\" : 0.08%\n",
      "的 : 0.04%\n",
      "。 : 0.01%\n",
      "Answer : 好\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_softmax(verb_hao[:10], 5, \"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_hao_acc = get_accuracy(verb_hao)\n",
    "verb_dao_acc = get_accuracy(verb_dao)\n",
    "verb_xialai_acc = get_accuracy(verb_xialai)\n",
    "verb_xiaqu_acc = get_accuracy(verb_xiaqu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb_hao accuracy :  0.6963\n",
      "Verb_dao accuracy :  0.7506\n",
      "Verb_xialai accuracy :  0.8381\n",
      "Verb_xiaqu accuracy :  0.8715\n"
     ]
    }
   ],
   "source": [
    "print(\"Verb_hao accuracy : \",verb_hao_acc)\n",
    "print(\"Verb_dao accuracy : \",verb_dao_acc)\n",
    "print(\"Verb_xialai accuracy : \",verb_xialai_acc)\n",
    "print(\"Verb_xiaqu accuracy : \",verb_xiaqu_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language model fine-tune (작업중)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "def load_data(sents, labels, batch_size=32, split=0.1):\n",
    "    # input\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")    \n",
    "    # attention\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    # label\n",
    "    label_ids = [tokenizer.convert_tokens_to_ids(x) for x in labels]\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    label_ids, \n",
    "                                                                                    random_state=1, \n",
    "                                                                                    test_size=0.1)\n",
    "    # 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=1, \n",
    "                                                       test_size=0.1)\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "    # 배치 사이즈\n",
    "    batch_size = 32\n",
    "\n",
    "    # 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "    # 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_sent, verb_label = preprocessing(verb_hao[:100])\n",
    "train_dataloader, val_dataloader = load_data(verb_sent, verb_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-d0b7e45cf18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 총 훈련 스텝 : 배치반복 횟수 * 에폭\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cn_base/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "optimizer = AdamW(model.parameters())\n",
    "epochs = 3\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        print(outputs)\n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
