{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from scikit-learn) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.19.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: keras in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /home/boychaboy/anaconda3/envs/cn_base/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from transformers import BertForMaskedLM,BertForSequenceClassification\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        l = []\n",
    "        for line in lines:\n",
    "            l.append(line.strip().split(\"</B>\")[1])\n",
    "#             print(line)\n",
    "    return l\n",
    "verb_hao = read_file(\"sample-data/verb_hao.txt\")\n",
    "verb_dao = read_file(\"sample-data/verb_dao.txt\")\n",
    "verb_xialai = read_file(\"sample-data/verb_xialai.txt\")\n",
    "verb_xiaqu = read_file(\"sample-data/verb_xiaqu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'bert-base-chinese'\n",
    "MAX_SIZE = 150\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentences) :\n",
    "    sent = []\n",
    "    label = []\n",
    "    for sentence in sentences :\n",
    "        \n",
    "        s1 = sentence.split(\"<U>\")\n",
    "        s2 = s1[1].split(\"</U>\")\n",
    "        label.append(s2[0][-1])\n",
    "        sent.append(\"[CLS]\"+s1[0]+s2[0][:-1]+\"[MASK]\"+s2[1]+\"[SEP]\")\n",
    "        #debug\n",
    "#         print(s2[0][-1]) # label\n",
    "#         print(s1[0]+s2[0][:-1]+\"[MASK]\"+s2[1]) # sent\n",
    "    return sent, label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 예측값의 확률 (Softmax) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_inference(sentences, top_n=5):\n",
    "    predicts = []\n",
    "    probabilities = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        mask_idx = input_ids.tolist()[0].index(103) #103 is [MASK] id\n",
    "        outputs = model(input_ids)\n",
    "        if top_n == 1:\n",
    "            logits = ouptuts[0].detach().numpy()\n",
    "            pred = tokenizer.convert_ids_to_tokens(np.argmax(logits, axis=2)[0])\n",
    "            predicts.append(pred[mask_idx])\n",
    "        else :\n",
    "            logits = outputs[0].detach()\n",
    "            mask_predict = F.softmax(logits[0][mask_idx], dim=0).numpy()\n",
    "            predict_top_n = mask_predict.argsort()[-top_n:][::-1]\n",
    "            predicts.append(tokenizer.convert_ids_to_tokens(predict_top_n))\n",
    "            probabilities.append(mask_predict[predict_top_n].tolist())\n",
    "    return predicts, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['下', '起', '过', '出', '上'] [0.8841641545295715, 0.11500083655118942, 0.0002584183239378035, 9.911341476254165e-05, 7.786255446262658e-05]\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "sents.append([\"这个东西[MASK]贵\"])\n",
    "sents.append([\"教室里突然安静[MASK]来了\"])\n",
    "pred, label = softmax_inference(sents)\n",
    "print(pred[1], label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred, label):\n",
    "    cnt = 0\n",
    "    for p, l in zip(pred, label):\n",
    "        if p==l : \n",
    "            cnt += 1\n",
    "    return cnt / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentences):\n",
    "    predicts = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        mask_idx = input_ids.tolist()[0].index(103) #103 is [MASK] id\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[0].detach().numpy()\n",
    "        pred = tokenizer.convert_ids_to_tokens(np.argmax(logits, axis=2)[0])\n",
    "        predicts.append(pred[mask_idx])\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data):\n",
    "    sent, label = preprocessing(data)\n",
    "    predict = inference(sent)\n",
    "    return evaluate(predict, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_softmax(data, top_n = 5, file_name=None):\n",
    "    sent, label = preprocessing(data)\n",
    "    pred, prob = softmax_inference(sent, top_n)\n",
    "    for sentence, candidates, probabilities, label in zip(sent, pred, prob, label):\n",
    "        print(sentence)\n",
    "        for c, p in zip(candidates,  probabilities):\n",
    "            print(\"{} : {:.2f}%\".format(c, p*100))\n",
    "        print(\"Answer : {}\".format(label))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]是约４２ｍｍ的遮光罩镜头外型尺寸为以说一尘不染，可见其密封性恨[MASK]。如此笔者发现独立镜头厂家适配０７２５ｘ１２２ｍｍ光学构造为１[SEP]\n",
      "好 : 43.59%\n",
      "佳 : 20.00%\n",
      "强 : 9.52%\n",
      "高 : 8.08%\n",
      "差 : 3.73%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]很爱听，真像换了个脑筋，明年养多少头牛、羊都一家一户地帮我们算[MASK]了。”东湾乡大泉村二组村民马建雄忙着拉干部到自家去住，“干部住[SEP]\n",
      "账 : 35.00%\n",
      "算 : 20.17%\n",
      "好 : 13.57%\n",
      "计 : 4.53%\n",
      "数 : 4.03%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]有关重大问题。第三，企业档案处置具体工作是收集、整理、统计、保管[MASK]企业已经形成的档案，清点库存，按有关规定做好档案留存与销毁的鉴[SEP]\n",
      "各 : 35.11%\n",
      "本 : 31.27%\n",
      "好 : 7.47%\n",
      "该 : 5.38%\n",
      "出 : 2.22%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]策与财政政策的协调配合还体现为在经济发展的不同时期，两者都要搭配[MASK]。１９９８年下半年，财政为扩大内需，向国有商业银行增加发行１０[SEP]\n",
      "好 : 72.64%\n",
      "住 : 6.52%\n",
      "上 : 5.45%\n",
      "合 : 3.22%\n",
      "着 : 2.45%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]政府贯彻〈中国教育改革和发展纲要〉的意见》。发展教育事业注意处理[MASK]三个关系：在数量和质量的关系上，更加重视质量；在德育和智育的关[SEP]\n",
      "好 : 97.80%\n",
      "了 : 0.46%\n",
      "这 : 0.42%\n",
      "在 : 0.33%\n",
      "的 : 0.32%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]学管理的示范。一是要通过学习，增强创新创业的自觉性。要创业、干[MASK]事业，就必须抓重点、带全局。所有的人力、智力、财力、物力、科技[SEP]\n",
      "好 : 93.50%\n",
      "大 : 3.51%\n",
      "新 : 1.03%\n",
      "实 : 0.44%\n",
      "事 : 0.36%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]解放思想，寻找经济快速发展的新路子。只有人的思想解放了，才能运用[MASK]中央制定的各项方针和政策，才能敢“闯”敢“新”，走出一条独特的[SEP]\n",
      "好 : 92.17%\n",
      "党 : 5.79%\n",
      "住 : 1.31%\n",
      "新 : 0.18%\n",
      "了 : 0.11%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]到手术室，手术室大空间，手术床小范围，手术室到ＩＣＵ等各环节做[MASK]保温，避免外介温度的大波动引起婴儿不良反应。３．３减少患婴术中[SEP]\n",
      "好 : 97.95%\n",
      "到 : 0.87%\n",
      "足 : 0.13%\n",
      "预 : 0.06%\n",
      "的 : 0.06%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]例如，在学习“如何做小主人和小客人”这一社会知识时，我们事先联系[MASK]一个家庭，并向主人详细讲解此次活动的目的、方法、步骤，以取得默[SEP]\n",
      "了 : 32.38%\n",
      "上 : 15.16%\n",
      "好 : 13.83%\n",
      "到 : 10.66%\n",
      "每 : 8.36%\n",
      "Answer : 好\n",
      "\n",
      "[CLS]切实加强农村信用社金融监管与行业管理的初步设想（一）必须正确处理[MASK]三大关系回、正确处理好信用社与联社的关系。基本原则是既要尊重信[SEP]\n",
      "好 : 99.53%\n",
      "这 : 0.27%\n",
      "\" : 0.08%\n",
      "的 : 0.04%\n",
      "。 : 0.01%\n",
      "Answer : 好\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_softmax(verb_hao[:10], 5, \"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_hao_acc = get_accuracy(verb_hao)\n",
    "verb_dao_acc = get_accuracy(verb_dao)\n",
    "verb_xialai_acc = get_accuracy(verb_xialai)\n",
    "verb_xiaqu_acc = get_accuracy(verb_xiaqu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb_hao accuracy :  0.6963\n",
      "Verb_dao accuracy :  0.7506\n",
      "Verb_xialai accuracy :  0.8381\n",
      "Verb_xiaqu accuracy :  0.8715\n"
     ]
    }
   ],
   "source": [
    "print(\"Verb_hao accuracy : \",verb_hao_acc)\n",
    "print(\"Verb_dao accuracy : \",verb_dao_acc)\n",
    "print(\"Verb_xialai accuracy : \",verb_xialai_acc)\n",
    "print(\"Verb_xiaqu accuracy : \",verb_xiaqu_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
